
# Điều Chỉnh Siêu Tham Số và Các Tính Năng Nâng cao của SageMaker

[Phần nội dung trước đó giữ nguyên]

## Đào tạo Phân tán trong SageMaker

Đào tạo phân tán là cần thiết khi mô hình quá lớn để đào tạo trên một máy đơn lẻ. SageMaker cung cấp nhiều tùy chọn để thực hiện đào tạo phân tán hiệu quả.

### Loại Đào tạo Phân tán

1. Job Parallelism:
   - Đào tạo nhiều mô hình khác nhau song song
   - Phù hợp khi cần đào tạo nhiều mô hình độc lập

2. Distributed Data Parallelism:
   - Phân phối dữ liệu đào tạo trên nhiều instance
   - Phù hợp khi dữ liệu đào tạo rất lớn

3. Distributed Model Parallelism:
   - Phân phối mô hình trên nhiều instance
   - Cần thiết khi mô hình quá lớn để vừa trên một GPU đơn lẻ

### Lưu ý quan trọng

- Tối ưu hóa sử dụng instance đơn lẻ trước khi chuyển sang đào tạo phân tán
- Sử dụng instance lớn (như ML p4d.24xlarge với 8 GPUs) trước khi chuyển sang nhiều instance
- Đào tạo phân tán có thể tốn kém, cần cân nhắc chi phí và hiệu suất

### Thư viện Đào tạo Phân tán của SageMaker

SageMaker cung cấp thư viện đào tạo phân tán dựa trên AWS Custom Collective library cho EC2.

1. All-Reduce Collective:
   - Phân phối tính toán cập nhật gradient giữa các GPU
   - Là một phần của SageMaker Distributed Data Parallelism (DDP) library
   - Cách sử dụng:
     1. Chỉ định backend `smdp` cho `torch.distributed.init_process_group` trong script đào tạo
     2. Đặt `distribution='smdistributed_dataparallel'` trong PyTorch estimator

2. Allgather Collective:
   - Quản lý giao tiếp giữa các node trong cluster
   - Cải thiện hiệu suất bằng cách chuyển gánh nặng giao tiếp sang CPU

Lưu ý: Không tương thích với SageMaker Training Compiler

### Các Thư viện Đào tạo Phân tán Khác

SageMaker hỗ trợ nhiều thư viện đào tạo phân tán khác:

1. PyTorch Distributed Data Parallel (DDP):
   - Cách sử dụng: Đặt `distribution='pytorch_ddp'` trong estimator

2. Torch Run:
   - Cách sử dụng: Đặt `distribution='torch_distributed'`
   - Yêu cầu instance P3, P4 hoặc Trn1

3. MPI Run

4. DeepSpeed:
   - Framework đào tạo phân tán mã nguồn mở từ Microsoft
   - Chỉ hỗ trợ PyTorch

5. Horovod:
   - Framework đào tạo phân tán phổ biến khác

## Kết luận

Đào tạo phân tán trong SageMaker cung cấp nhiều tùy chọn để xử lý các mô hình và bộ dữ liệu cực lớn. Từ thư viện tích hợp của SageMaker đến các framework phổ biến như PyTorch DDP và DeepSpeed, người dùng có thể chọn giải pháp phù hợp nhất với nhu cầu của họ. Tuy nhiên, việc sử dụng đào tạo phân tán cần được cân nhắc kỹ lưỡng về mặt chi phí và hiệu suất, và nên được áp dụng chỉ khi đã tối ưu hóa việc sử dụng các instance đơn lẻ.

# Activation Functions trong Neural Networks

## 1. Tổng quan và Phân loại

```mermaid
graph TB
    A[Activation Functions] --> B[Linear]
    A --> C[Non-linear]
    
    B --> D[Linear Function]
    B --> E[Binary Step]
    
    C --> F[Sigmoid/Tanh]
    C --> G[ReLU Family]
    C --> H[Special Functions]
    
    G --> I[ReLU]
    G --> J[Leaky ReLU]
    G --> K[PReLU]
    
    H --> L[Softmax]
    H --> M[Swish]
    H --> N[Maxout]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B,C fill:#bbf,stroke:#333
```

## 2. Linear Activation Functions

### 2.1 Linear Function
- **Đặc điểm:** Output = Input
- **Vấn đề:**
  - Không học được mẫu phức tạp
  - Không hỗ trợ backpropagation
  - Vô nghĩa với multiple layers

### 2.2 Binary Step
- **Đặc điểm:** Output chỉ 0 hoặc 1
- **Vấn đề:**
  - Không xử lý được multiple classification
  - Đạo hàm vô cùng tại điểm chuyển
  - Không ổn định về mặt toán học

## 3. Non-linear Activation Functions

### 3.1 Sigmoid và Tanh
```mermaid
graph LR
    A[Sigmoid/Tanh] --> B[Sigmoid: 0 to 1]
    A --> C[Tanh: -1 to 1]
    
    D[Issues] --> E[Vanishing Gradient]
    D --> F[Computationally Expensive]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#bbf,stroke:#333
```

### 3.2 ReLU Family

1. **ReLU:**
   - Đơn giản và hiệu quả
   - Tính toán nhanh
   - Vấn đề "dying ReLU"

2. **Leaky ReLU:**
   - Giải quyết dying ReLU
   - Slope âm nhỏ
   - Slope cố định

3. **PReLU:**
   - Slope âm học được
   - Phức tạp hơn
   - Tốn nhiều tài nguyên

### 3.3 Special Functions

1. **Softmax:**
   - Dùng cho output layer
   - Multiple classification
   - Chuyển đổi sang probabilities

2. **Swish:**
   - Phát triển bởi Google
   - Tốt cho deep networks (>40 layers)
   - Smooth curve

3. **Maxout:**
   - Output là max của inputs
   - Gấp đôi parameters
   - Tốn kém về tính toán

## 4. Hướng dẫn lựa chọn

```mermaid
graph TB
    A[Use Case] --> B[Multiple Classification]
    A --> C[RNN]
    A --> D[Deep Networks]
    A --> E[Multiple Labels]
    
    B --> F[Softmax]
    C --> G[Tanh]
    D --> H[ReLU/Swish]
    E --> I[Sigmoid]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

### 4.1 Quy trình lựa chọn:
1. **Output layer:**
   - Multiple classification → Softmax
   - Multiple labels → Sigmoid

2. **Hidden layers:**
   - RNN → Tanh
   - Basic → ReLU
   - Performance issues → Leaky ReLU
   - Deep networks → Swish

## 5. Lưu ý quan trọng cho kỳ thi

1. **Hiểu rõ:**
   - Vấn đề của linear functions
   - Ưu điểm của non-linear functions
   - Use cases của mỗi loại

2. **Nhớ rằng:**
   - ReLU là lựa chọn phổ biến nhất
   - Softmax cho classification
   - Tanh cho RNN
   - Sigmoid cho multiple labels

3. **Không cần:**
   - Chi tiết công thức toán học
   - Cách implement
   - Các biến thể ít phổ biến
# Tuning Neural Networks - Learning Rate và Batch Size

## 1. Tổng quan về Gradient Descent

```mermaid
graph TB
    A[Gradient Descent] --> B[Random Initial Weights]
    A --> C[Multiple Epochs]
    A --> D[Cost Function]
    
    B --> E[Sample Solutions]
    C --> F[Iterative Learning]
    D --> G[Optimization Goal]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B,C,D fill:#bbf,stroke:#333
```

## 2. Learning Rate

### 2.1 Khái niệm
```mermaid
graph LR
    A[Learning Rate] --> B[Too High]
    A --> C[Too Low]
    
    B --> D[Overshoot]
    B --> E[Miss Solution]
    
    C --> F[Slow Training]
    C --> G[More Epochs]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

### 2.2 Ảnh hưởng:
1. **Learning Rate Cao:**
   - Nhảy quá xa giữa các steps
   - Có thể bỏ qua giải pháp tối ưu
   - Không ổn định trong training

2. **Learning Rate Thấp:**
   - Training chậm hơn
   - Cần nhiều epochs hơn
   - Tốn thời gian và tài nguyên

## 3. Batch Size

### 3.1 Quan hệ với Local Minima

```mermaid
graph TB
    A[Batch Size] --> B[Small Batch]
    A --> C[Large Batch]
    
    B --> D[Escape Local Minima]
    B --> E[Better Exploration]
    
    C --> F[Stuck in Local Minima]
    C --> G[Inconsistent Results]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

### 3.2 Đặc điểm:
1. **Batch Size Nhỏ:**
   - Dễ thoát local minima
   - Linh hoạt trong exploration
   - Kết quả ổn định hơn

2. **Batch Size Lớn:**
   - Dễ mắc kẹt tại local minima
   - Kết quả không nhất quán
   - Random convergence

## 4. Mối quan hệ và Trade-offs

### 4.1 Learning Rate vs Performance
```mermaid
graph LR
    A[Performance] --> B[Speed]
    A --> C[Accuracy]
    
    B --> D[Higher Learning Rate]
    C --> E[Lower Learning Rate]
    
    D --> F[Risk Overshooting]
    E --> G[Slower Training]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

### 4.2 Batch Size vs Stability
```mermaid
graph LR
    A[Stability] --> B[Exploration]
    A --> C[Convergence]
    
    B --> D[Small Batch]
    C --> E[Large Batch]
    
    D --> F[Better Solutions]
    E --> G[Risk Local Minima]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

## 5. Lưu ý quan trọng cho kỳ thi

### 5.1 Learning Rate:
1. **High Learning Rate:**
   - CÓ THỂ: Bỏ lỡ giải pháp tối ưu
   - CÓ THỂ: Training không ổn định

2. **Low Learning Rate:**
   - CÓ THỂ: Training quá lâu
   - NHƯNG: Kết quả ổn định hơn

### 5.2 Batch Size:
1. **Small Batch Size:**
   - TỐT: Thoát local minima
   - TỐT: Kết quả nhất quán
   - TỐT: Tìm giải pháp tốt hơn

2. **Large Batch Size:**
   - XẤU: Mắc kẹt local minima
   - XẤU: Kết quả không nhất quán
   - XẤU: Có thể hội tụ sai

## 6. Best Practices

### 6.1 Tuning Strategy:
1. **Bắt đầu với:**
   - Learning rate vừa phải
   - Batch size nhỏ

2. **Điều chỉnh khi:**
   - Training quá chậm → Tăng learning rate
   - Kết quả không ổn định → Giảm batch size

### 6.2 Monitoring:
1. **Theo dõi:**
   - Loss function
   - Convergence speed
   - Result consistency

2. **Red flags:**
   - Kết quả không nhất quán
   - Training không hội tụ
   - Overshooting rõ ràng